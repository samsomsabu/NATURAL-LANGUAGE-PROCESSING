{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMRsFYsSl7Q3CBZEy/Gum4Q",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/samsomsabu/NATURAL-LANGUAGE-PROCESSING/blob/main/Untitled64.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1. Write a program to get Antonyms from WordNet.**"
      ],
      "metadata": {
        "id": "KKyj4_Em55FJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Explanation***"
      ],
      "metadata": {
        "id": "hACz6cJjFXGS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "WordNet is a large lexical database of English. Nouns, verbs, adjectives and adverbs are grouped into sets of cognitive synonyms (synsets), each expressing a distinct concept. Synsets are interlinked by means of conceptual-semantic and lexical relations.\n",
        "\n",
        "\n",
        "WordNet's structure makes it a useful tool for computational linguistics and natural language processing.\n",
        "\n",
        "WordNet superficially resembles a thesaurus, in that it groups words together based on their meanings. However, there are some important distinctions.\n",
        "\n",
        "First, WordNet interlinks not just word forms—strings of letters—but specific senses of words. As a result, words that are found in close proximity to one another in the network are semantically disambiguated.\n",
        "Second, WordNet labels the semantic relations among words, whereas the groupings of words in a thesaurus does not follow any explicit pattern other than meaning similarity."
      ],
      "metadata": {
        "id": "IyXk6XEcFa5U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r-oAwcup6FFP",
        "outputId": "09a1bf31-aa0a-4912-c086-a685f5957934"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vK7FTKPW54dO",
        "outputId": "b246fbae-c7b0-4d0c-b3a6-c3757ee47a7f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'evilness', 'ill', 'badness', 'evil', 'bad'}\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.corpus import wordnet\n",
        "\n",
        "antonyms = []\n",
        "\n",
        "for syn in wordnet.synsets(\"good\"):\n",
        "    for l in syn.lemmas():\n",
        "        if l.antonyms():\n",
        "            antonyms.append(l.antonyms()[0].name())\n",
        "\n",
        "print(set(antonyms))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "antonyms = []: Initializes an empty list called antonyms to store the antonyms found for the word \"good\".\n",
        "\n",
        "for syn in wordnet.synsets(\"good\"):: This loop iterates through all the synsets (sets of synonyms) in WordNet for the word \"good\".\n",
        "\n",
        "for l in syn.lemmas():: This nested loop iterates through all the lemmas (words with the same meaning) in each synset.\n",
        "\n",
        "if l.antonyms(): Checks if the current lemma has any antonyms.\n",
        "\n",
        "antonyms.append(l.antonyms()[0].name()): If an antonym exists for the current lemma, the first antonym found is added to the antonyms list.\n",
        "\n",
        "print(set(antonyms)): Finally, the set of antonyms found is printed to the console. Using set ensures that duplicates are removed, providing a unique list of antonyms for the word \"good.\""
      ],
      "metadata": {
        "id": "dbw_mRBPGM7t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import wordnet\n",
        "\n",
        "antonyms = []\n",
        "\n",
        "for syn in wordnet.synsets(\"love\"):\n",
        "    for i in syn.lemmas():\n",
        "         if i.antonyms():\n",
        "              antonyms.append(i.antonyms()[0].name())\n",
        "\n",
        "print(set(antonyms))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NKKDJvlJ5-eF",
        "outputId": "b7e1a785-b58a-4c0d-a1fb-3f4f600d9975"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'hate'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://www.geeksforgeeks.org/get-synonymsantonyms-nltk-wordnet-python/"
      ],
      "metadata": {
        "id": "P0gl5lm46lqh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://www.holisticseo.digital/python-seo/nltk/wordnet"
      ],
      "metadata": {
        "id": "YR0qkLmU6oXE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Write a program for stemming non-English words."
      ],
      "metadata": {
        "id": "KaTXEyYp6N5_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stemming\n",
        "\n",
        "Stemming is a technique used to reduce an inflected word down to its word stem. For example, the words “programming,” “programmer,” and “programs” can all be reduced down to the common word stem “program.” In other words, “program” can be used as a synonym for the prior three inflection words.\n",
        "\n",
        "Performing this text-processing technique is often useful for dealing with sparsity and/or standardizing vocabulary. Not only does it help with reducing redundancy, as most of the time the word stem and their inflected words have the same meaning, it also allows NLP models to learn links between inflected words and their word stem, which helps the model understand their usage in similar contexts.\n",
        "\n",
        "Stemming algorithms function by taking a list of frequent prefixes and suffixes found in inflected words and chopping off the end or beginning of the word. This can occasionally result in word stems that are not real words; thus, we can affirm this approach certainly has its pros, but it’s not without its limitations."
      ],
      "metadata": {
        "id": "b-GzVu3fGis9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "nWOkbaCrGjzf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem.snowball import GermanStemmer"
      ],
      "metadata": {
        "id": "LTP8dBi76KEy"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "german_st = GermanStemmer()"
      ],
      "metadata": {
        "id": "KYKYb-al6SbX"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "token_sample = [\"Schreiben\",\"geschrieben\"]"
      ],
      "metadata": {
        "id": "acth3UGy6VWl"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stem_words = [german_st.stem(words) for words in token_sample]"
      ],
      "metadata": {
        "id": "646YoGM06XwL"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Print the output after stemming:\",stem_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t6KtEl736aIf",
        "outputId": "c1834902-0ab4-4bec-ce63-f92e81cfc7ce"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Print the output after stemming: ['schreib', 'geschrieb']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 1 - Import the German language Stemmer\n",
        "\n",
        "\n",
        "Step 2 - Store the german stemmer in a variable\n",
        "\n",
        "\n",
        "Step 3 - Take sample words\n",
        "\n",
        "\n",
        "Step 4 - Apply stemming and print the results"
      ],
      "metadata": {
        "id": "PH5NfSfVGn7b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://www.projectpro.io/recipes/stem-non-english-words"
      ],
      "metadata": {
        "id": "f79Yv0NU6hhG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "SnowballStemmer class\n",
        "NLTK has SnowballStemmer class with the help of which we can easily implement Snowball Stemmer algorithms. It supports 15 non-English languages."
      ],
      "metadata": {
        "id": "GMvm7SKoGz3_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.stem import SnowballStemmer\n",
        "French_stemmer = SnowballStemmer('french')\n",
        "French_stemmer.stem ('Bonjoura')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "sDYUdYl0-5S7",
        "outputId": "b7fc3506-5954-461d-8ae9-699425aa5393"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'bonjour'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Write a program for lemmatizing words Using WordNet (Use all type of stemmers for the\n",
        "comparison)."
      ],
      "metadata": {
        "id": "B2OuVaVO6qq3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Lemmatization**\n",
        "Lemmatization is another technique used to reduce inflected words to their root word. It describes the algorithmic process of identifying an inflected word’s “lemma” (dictionary form) based on its intended meaning.\n",
        "\n",
        "As opposed to stemming, lemmatization relies on accurately determining the intended part-of-speech and the meaning of a word based on its context. This means it takes into consideration where the inflected word falls within a sentence, as well as within the larger context surrounding that sentence, such as neighboring sentences or even an entire document."
      ],
      "metadata": {
        "id": "X1Coj03CHIHV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.stem import PorterStemmer\n",
        "nltk.download(\"punkt\")\n",
        "# Initialize Python porter stemmer\n",
        "ps = PorterStemmer()\n",
        "# Example inflections to reduce\n",
        "example_words = [\"program\",\"programming\",\"programer\",\"programs\",\"programmed\"]\n",
        "# Perform stemming\n",
        "print(\"{0:20}{1:20}\".format(\"--Word--\",\"--Stem--\"))\n",
        "for word in example_words:\n",
        "   print (\"{0:20}{1:20}\".format(word, ps.stem(word)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cY9N2KgX8Fqu",
        "outputId": "99284700-7b93-43d0-a4c2-c53621f7e0c4"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--Word--            --Stem--            \n",
            "program             program             \n",
            "programming         program             \n",
            "programer           program             \n",
            "programs            program             \n",
            "programmed          program             \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code performs stemming on a list of example words using the Porter stemming algorithm from the Natural Language Toolkit (nltk) library in Python.\n",
        "\n",
        "\n",
        "• First, the nltk library is imported and the PorterStemmer class is imported from it.\n",
        "\n",
        "\n",
        "• Then, the \"punkt\" dataset is downloaded from nltk using the nltk.download() function.\n",
        "\n",
        "\n",
        "• Next, an instance of the PorterStemmer class is created and assigned to the variable ps.\n",
        "\n",
        "\n",
        "• A list of example words is defined in example_words.\n",
        "\n",
        "\n",
        "• A loop is then used to iterate through each word in example_words.\n",
        "\n",
        "\n",
        "• For each word, the ps.stem() method is called on it to perform stemming and the stemmed word is printed alongside the original word using the print() function.\n",
        "\n",
        "\n",
        "• The output shows the original words in the left column and their stemmed versions in the right column.\n",
        "\n",
        "\n",
        "• The Porter stemming algorithm reduces words to their base or root form by removing suffixes.\n",
        "\n",
        "\n",
        "• In this case, all the example words are stemmed to \"program\"."
      ],
      "metadata": {
        "id": "DXpS6IXnIxhe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download(\"wordnet\")\n",
        "nltk.download(\"omw-1.4\")\n",
        "# Initialize wordnet lemmatizer\n",
        "wnl = WordNetLemmatizer()\n",
        "# Example inflections to reduce\n",
        "example_words = [\"program\",\"programming\",\"programer\",\"programs\",\"programmed\"]\n",
        "# Perform lemmatization\n",
        "print(\"{0:20}{1:20}\".format(\"--Word--\",\"--Lemma--\"))\n",
        "for word in example_words:\n",
        "   print (\"{0:20}{1:20}\".format(word, wnl.lemmatize(word, pos=\"v\")))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XSowZQkK8eZZ",
        "outputId": "46be4f90-ed25-4b75-a0ff-cec9cbd74be4"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--Word--            --Lemma--           \n",
            "program             program             \n",
            "programming         program             \n",
            "programer           programer           \n",
            "programs            program             \n",
            "programmed          program             \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code imports the WordNetLemmatizer class from the nltk.stem module and downloads the necessary data for it to work using the nltk.download() function.\n",
        "\n",
        "\n",
        "• It then initializes an instance of the WordNetLemmatizer class and defines a list of example words to be lemmatized.\n",
        "\n",
        "\n",
        "• The code then performs lemmatization on each word in the list using the lemmatize() method of the WordNetLemmatizer class, with the optional argument pos=\"v\" indicating that the words should be lemmatized as verbs.\n",
        "\n",
        "\n",
        "• The results are printed in a formatted table with the original words in the left column and their lemmatized forms in the right column.\n",
        "\n"
      ],
      "metadata": {
        "id": "wsGvjSZ5I715"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.stem import LancasterStemmer\n",
        "Lanc_stemmer = LancasterStemmer()\n",
        "Lanc_stemmer.stem('books')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "y4KavtP3-sml",
        "outputId": "42fbd857-5a6d-4120-8596-13cf7d49fee9"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'book'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmatizer.lemmatize('books')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "hPO4T_xQ_qNc",
        "outputId": "d6fb20af-717d-42f9-9447-2e05f57e6951"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'book'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "import the WordNetLemmatizer class to implement the lemmatization technique.\n",
        "\n",
        "create an instance of WordNetLemmatizer class.\n",
        "\n",
        "call the lemmatize() method"
      ],
      "metadata": {
        "id": "wmtOTvReHp4-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://www.tutorialspoint.com/natural_language_toolkit/natural_language_toolkit_stemming_lemmatization.htm"
      ],
      "metadata": {
        "id": "e1pjHqSv8FWM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "example_sentence = \"Python programmers often tend like programming in python because it's like english. We call people who program in python pythonistas.\"\n",
        "\n",
        "# Remove punctuation\n",
        "example_sentence_no_punct = example_sentence.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
        "\n",
        "# Create tokens\n",
        "word_tokens = word_tokenize(example_sentence_no_punct)\n",
        "\n",
        "# Perform stemming\n",
        "print(\"{0:20}{1:20}\".format(\"--Word--\",\"--Stem--\"))\n",
        "for word in word_tokens:\n",
        "    print (\"{0:20}{1:20}\".format(word, ps.stem(word)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8i0fl__x8U1-",
        "outputId": "cd1e56e0-d6ab-42b6-b59e-fb2f19ab7269"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--Word--            --Stem--            \n",
            "Python              python              \n",
            "programmers         programm            \n",
            "often               often               \n",
            "tend                tend                \n",
            "like                like                \n",
            "programming         program             \n",
            "in                  in                  \n",
            "python              python              \n",
            "because             becaus              \n",
            "its                 it                  \n",
            "like                like                \n",
            "english             english             \n",
            "We                  we                  \n",
            "call                call                \n",
            "people              peopl               \n",
            "who                 who                 \n",
            "program             program             \n",
            "in                  in                  \n",
            "python              python              \n",
            "pythonistas         pythonista          \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code is a Python script that demonstrates how to remove punctuation, tokenize a sentence, and perform stemming using the Natural Language Toolkit (nltk) library.\n",
        "\n",
        "\n",
        "• First, the script imports the string and word_tokenize modules from the nltk library.\n",
        "\n",
        "\n",
        "• It also defines an example sentence as a string variable.\n",
        "\n",
        "\n",
        "• Next, the script removes the punctuation from the example sentence using the translate() method and the str.maketrans() function.\n",
        "\n",
        "\n",
        "• This creates a new string variable called example_sentence_no_punct.\n",
        "\n",
        "\n",
        "• After that, the script tokenizes the example_sentence_no_punct variable using the word_tokenize() method.\n",
        "\n",
        "\n",
        "• This creates a list of word tokens called word_tokens.\n",
        "\n",
        "\n",
        "• Finally, the script performs stemming on each word in the word_tokens list using the PorterStemmer algorithm from the nltk library.\n",
        "\n",
        "\n",
        "• It then prints out each word and its corresponding stem using a formatted string.\n",
        "\n",
        "\n",
        "• Overall, this code demonstrates how to use the nltk library to preprocess text data by removing punctuation, tokenizing sentences, and performing stemming."
      ],
      "metadata": {
        "id": "PZm2Y9iLIT1P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "example_sentence = \"Python programmers often tend like programming in python because it's like english. We call people who program in python pythonistas.\"\n",
        "# Remove punctuation\n",
        "example_sentence_no_punct = example_sentence.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
        "word_tokens = word_tokenize(example_sentence_no_punct)\n",
        "# Perform lemmatization\n",
        "print(\"{0:20}{1:20}\".format(\"--Word--\",\"--Lemma--\"))\n",
        "for word in word_tokens:\n",
        "   print (\"{0:20}{1:20}\".format(word, wnl.lemmatize(word, pos=\"v\")))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m6yya3_C8dHM",
        "outputId": "1fc7cd0b-9d47-47c2-9526-8aa7d80bdd2a"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--Word--            --Lemma--           \n",
            "Python              Python              \n",
            "programmers         programmers         \n",
            "often               often               \n",
            "tend                tend                \n",
            "like                like                \n",
            "programming         program             \n",
            "in                  in                  \n",
            "python              python              \n",
            "because             because             \n",
            "its                 its                 \n",
            "like                like                \n",
            "english             english             \n",
            "We                  We                  \n",
            "call                call                \n",
            "people              people              \n",
            "who                 who                 \n",
            "program             program             \n",
            "in                  in                  \n",
            "python              python              \n",
            "pythonistas         pythonistas         \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code performs text preprocessing on a given sentence.\n",
        "\n",
        "\n",
        "• First, the sentence is assigned to a variable called example_sentence.\n",
        "\n",
        "\n",
        "• Then, the translate() method is used to remove all punctuation from the sentence.\n",
        "\n",
        "\n",
        "• This is done by calling the maketrans() method from the string module to create a translation table that maps all punctuation characters to None.\n",
        "\n",
        "\n",
        "• The resulting string with no punctuation is assigned to a new variable called example_sentence_no_punct.\n",
        "\n",
        "\n",
        "• Next, the word_tokenize() method from the nltk module is used to tokenize the sentence into individual words.\n",
        "\n",
        "\n",
        "• The resulting list of words is assigned to a variable called word_tokens.\n",
        "\n",
        "• Finally, the code performs lemmatization on each word in the word_tokens list using the lemmatize() method from the WordNetLemmatizer class in the nltk module.\n",
        "\n",
        "\n",
        "• The lemmatized word and its original form are printed in a formatted table using the print() function."
      ],
      "metadata": {
        "id": "6-NsnlzhJIa2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://www.datacamp.com/tutorial/stemming-lemmatization-python"
      ],
      "metadata": {
        "id": "BUGnzWZa8FZu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Write a program to differentiate stemming and lemmatizing words."
      ],
      "metadata": {
        "id": "T67Gxxmo7OoS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import nltk\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Initialize stemmer and lemmatizer\n",
        "stemmer = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Input word\n",
        "word = input(\"Enter a word: \")\n",
        "\n",
        "# Stemming\n",
        "stemmed_word = stemmer.stem(word)\n",
        "\n",
        "# Lemmatizing\n",
        "lemmatized_word = lemmatizer.lemmatize(word, pos='v')  # Specify 'v' for verb lemmatization\n",
        "\n",
        "# Display the results\n",
        "print(f\"Word: {word}\")\n",
        "print(f\"Stemmed Word: {stemmed_word}\")\n",
        "print(f\"Lemmatized Word: {lemmatized_word}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "87plF0Dd7lor",
        "outputId": "95d41d66-5eee-4195-9dfb-4977dc73dea6"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter a word: sams\n",
            "Word: sams\n",
            "Stemmed Word: sam\n",
            "Lemmatized Word: sams\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Write a program for PoS Tagging and also execute any of the tool that given in class."
      ],
      "metadata": {
        "id": "3bwHdQc7_6QP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy\n",
        "!python -m spacy download en_core_web_sm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qdcbBCzsEsOA",
        "outputId": "a0162a40-58e0-4ab4-91a5-dcacc5d64d3d"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.7.4)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.2.3)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.3.4)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.9.0)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (4.66.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.6.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (24.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.3.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.25.2)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.16.3)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.10.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.2.2)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy) (8.1.7)\n",
            "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.4.0,>=0.1.0->spacy) (0.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (2.1.5)\n",
            "Collecting en-core-web-sm==3.7.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m28.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /usr/local/lib/python3.10/dist-packages (from en-core-web-sm==3.7.1) (3.7.4)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.3)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.3.4)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.9.0)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.66.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.6.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (24.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.25.2)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.16.3)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.10.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2024.2.2)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\n",
            "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.5)\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#importing libraries\n",
        "import spacy\n",
        "\n",
        "# Load the English language model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "text = \"My name is Samson Sabu . I hail from the serene town named Nedumkandam.\"\n",
        "\n",
        "# Process the text with SpaCy\n",
        "doc = nlp(text)\n",
        "\n",
        "# Display the PoS tagged result\n",
        "print(\"Original Text: \", text)\n",
        "print(\"PoS Tagging Result:\")\n",
        "for token in doc:\n",
        "\tprint(f\"{token.text}: {token.pos_}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-K3P3bRVESqw",
        "outputId": "94a7e1df-3f77-45f8-a8ef-72b229edba2c"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Text:  My name is Samson Sabu . I hail from the serene town named Nedumkandam.\n",
            "PoS Tagging Result:\n",
            "My: PRON\n",
            "name: NOUN\n",
            "is: AUX\n",
            "Samson: PROPN\n",
            "Sabu: PROPN\n",
            ".: PUNCT\n",
            "I: PRON\n",
            "hail: VERB\n",
            "from: ADP\n",
            "the: DET\n",
            "serene: ADJ\n",
            "town: NOUN\n",
            "named: VERB\n",
            "Nedumkandam: PROPN\n",
            ".: PUNCT\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://www.geeksforgeeks.org/nlp-part-of-speech-default-tagging/"
      ],
      "metadata": {
        "id": "GETU9h2nE327"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **POS Tagging**\n",
        "POS Tagging (Parts of Speech Tagging) is a process to mark up the words in text format for a particular part of a speech based on its definition and context.\n",
        "\n",
        " It is responsible for text reading in a language and assigning some specific token (Parts of Speech) to each word. It is also called grammatical tagging."
      ],
      "metadata": {
        "id": "9-lvetUpJ2RM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **What is Chunking in NLP?**\n",
        "Chunking in NLP is a process to take small pieces of information and group them into large units. The primary use of Chunking is making groups of “noun phrases.” It is used to add structure to the sentence by following POS tagging combined with regular expressions. The resulted group of words are called “chunks.” It is also called shallow parsing.\n",
        "\n",
        "In shallow parsing, there is maximum one level between roots and leaves while deep parsing comprises of more than one level. Shallow parsing is also called light parsing or chunking."
      ],
      "metadata": {
        "id": "c-PPPMFuKBSj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import pos_tag\n",
        "from nltk import RegexpParser\n",
        "text =\"My name is Samson Sabu . I hail from the serene town named Nedumkandam.\".split()\n",
        "print(\"After Split:\",text)\n",
        "tokens_tag = pos_tag(text)\n",
        "print(\"After Token:\",tokens_tag)\n",
        "patterns= \"\"\"mychunk:{<NN.?>*<VBD.?>*<JJ.?>*<CC>?}\"\"\"\n",
        "chunker = RegexpParser(patterns)\n",
        "print(\"After Regex:\",chunker)\n",
        "output = chunker.parse(tokens_tag)\n",
        "print(\"After Chunking\",output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N_wsr3y0EtP1",
        "outputId": "a6a7d639-fa67-4b5d-e410-0476201ef6a4"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After Split: ['My', 'name', 'is', 'Samson', 'Sabu', '.', 'I', 'hail', 'from', 'the', 'serene', 'town', 'named', 'Nedumkandam.']\n",
            "After Token: [('My', 'PRP$'), ('name', 'NN'), ('is', 'VBZ'), ('Samson', 'NNP'), ('Sabu', 'NNP'), ('.', '.'), ('I', 'PRP'), ('hail', 'VBP'), ('from', 'IN'), ('the', 'DT'), ('serene', 'JJ'), ('town', 'NN'), ('named', 'VBN'), ('Nedumkandam.', 'NNP')]\n",
            "After Regex: chunk.RegexpParser with 1 stages:\n",
            "RegexpChunkParser with 1 rules:\n",
            "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
            "After Chunking (S\n",
            "  My/PRP$\n",
            "  (mychunk name/NN)\n",
            "  is/VBZ\n",
            "  (mychunk Samson/NNP Sabu/NNP)\n",
            "  ./.\n",
            "  I/PRP\n",
            "  hail/VBP\n",
            "  from/IN\n",
            "  the/DT\n",
            "  (mychunk serene/JJ)\n",
            "  (mychunk town/NN)\n",
            "  named/VBN\n",
            "  (mychunk Nedumkandam./NNP))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "bJv4tcemL23R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install stanza\n"
      ],
      "metadata": {
        "id": "DhVssA42L3cM",
        "outputId": "e46cf03d-15e9-475b-80de-fdbbdccd3e71",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting stanza\n",
            "  Downloading stanza-1.8.1-py3-none-any.whl (970 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m970.4/970.4 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting emoji (from stanza)\n",
            "  Downloading emoji-2.10.1-py2.py3-none-any.whl (421 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m421.5/421.5 kB\u001b[0m \u001b[31m35.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from stanza) (1.25.2)\n",
            "Requirement already satisfied: protobuf>=3.15.0 in /usr/local/lib/python3.10/dist-packages (from stanza) (3.20.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from stanza) (2.31.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from stanza) (3.2.1)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.10/dist-packages (from stanza) (0.10.2)\n",
            "Requirement already satisfied: torch>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from stanza) (2.2.1+cu121)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from stanza) (4.66.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->stanza) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->stanza) (4.10.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->stanza) (1.12)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->stanza) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->stanza) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.3.0->stanza)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m38.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.3.0->stanza)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m56.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.3.0->stanza)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m47.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.3.0->stanza)\n",
            "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━\u001b[0m \u001b[32m509.2/731.7 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:01:54\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import stanza\n",
        "\n",
        "# Download the CoreNLP English model\n",
        "stanza.download('en')\n",
        "\n",
        "# Initialize the CoreNLP pipeline\n",
        "nlp = stanza.Pipeline('en', processors='tokenize,ner')\n",
        "\n",
        "# Sample text for analysis\n",
        "text = \"My name is Samson Sabu . I hail from the serene town named Nedumkandam.\"\n",
        "# Process the text using the NER model from CoreNLP\n",
        "doc = nlp(text)\n",
        "\n",
        "# Iterate through the recognized entities and print them\n",
        "for sent in doc.sentences:\n",
        "    for ent in sent.ents:\n",
        "        print(f\"{ent.text}\\t{ent.type}\")\n"
      ],
      "metadata": {
        "id": "10CZhihFL7q0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Implement the Dependency Parsing and Constituency Parsing using the tool."
      ],
      "metadata": {
        "id": "zTsORBQOLlwi"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "W60Mi8s4Ki39"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}